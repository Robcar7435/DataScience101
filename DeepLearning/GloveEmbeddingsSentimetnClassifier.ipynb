{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU avialible: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/matthew/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    " ## Deep Learning Sentiment Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import OrderedDict\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Dropout, Reshape, Activation, Input, Flatten, Dense, RepeatVector, BatchNormalization, LSTM, RepeatVector, MaxPooling1D,GlobalMaxPool1D, Conv1D, GRU, Bidirectional\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "import requests \n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "print('GPU avialible: {}'.format(tf.test.is_gpu_available()))\n",
    "pd.options.display.width = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bruce willis and sixth sense director m . night shyamalan re-team to tell the story of david dunne ( willis ) , a stadium security guard who has been having some problems at home that are affecting hi\n",
      "label: 1\n",
      "n_docs 2000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Download Cornal Moview revirew data sets\n",
    "url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz'\n",
    "file_path = 'polarity.tar.gz'\n",
    "path = Path(file_path)\n",
    "response = requests.get(url)\n",
    "\n",
    "# write contents of the download \n",
    "path.write_bytes(response.content)\n",
    "\n",
    "# unziped the file create a directory called txt_sentoken\n",
    "os.system('tar -xzf {}'.format(file_path))\n",
    "\n",
    "# Note if you don't have tar, you may need to use 7zip or another unzipping tool (windows)\n",
    "# set positive and negative directory paths\n",
    "pos_dir = 'txt_sentoken/pos'\n",
    "neg_dir = 'txt_sentoken/neg'\n",
    "\n",
    "# define a function to read in text\n",
    "def read_file_list(file_list):\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.readlines()\n",
    "            text = ' '.join(text)\n",
    "            yield text\n",
    "            \n",
    "# get the list of file paths\n",
    "pos_files = os.listdir(pos_dir)\n",
    "neg_files = os.listdir(neg_dir)\n",
    "\n",
    "\n",
    "# data the working dir to the file paths\n",
    "pos_files = [pos_dir + '/' + f for f in pos_files if 'cv' in f]\n",
    "neg_files = [neg_dir + '/' + f for f in neg_files if 'cv' in f]\n",
    "neg_reviews = list(read_file_list(neg_files))\n",
    "pos_reviews = list(read_file_list(pos_files))\n",
    "data = pos_reviews + neg_reviews\n",
    "\n",
    "# positive 1, negative review 5\n",
    "labels = np.array([1] * len(pos_reviews) + [0] * len(neg_reviews))\n",
    "\n",
    "#\n",
    "print(data[1][:200])\n",
    "print('label:', labels[1])\n",
    "print('n_docs {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size 1000, max_len 1000\n"
     ]
    }
   ],
   "source": [
    "# sets the max number of words in the tokenizer\n",
    "vocab_size = 1000\n",
    "max_len = 1000\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "\n",
    "# fits on tokenizer\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "print('vocab_size {0}, max_len {1}'.format(vocab_size, max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load embeddings with 100 vector inputs\n",
    "path_to_glove_embeddings = '/home/matthew/repos/glove/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "# loads glove embeddings ()\n",
    "f = open(path_to_glove_embeddings, encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_weights shape (1001, 100)\n",
      "2.9% not found\n"
     ]
    }
   ],
   "source": [
    "# create embedding weight array (looks up words from the tokenizers vocab)\n",
    "dim_embedddings = 100\n",
    "not_found = []\n",
    "def get_emb(word_index):\n",
    "    found = 0\n",
    "    missed = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i <= vocab_size:\n",
    "            try:\n",
    "                emb = embeddings_index[word]\n",
    "            except KeyError:\n",
    "                emb = np.zeros(dim_embedddings)\n",
    "                not_found.append(word)\n",
    "            yield emb\n",
    "\n",
    "# adds an array of zeros for when word index is 0\n",
    "embedding_weights = np.array([np.zeros(dim_embedddings)] + list(get_emb(tokenizer.word_index)))\n",
    "print('embedding_weights shape {}'.format(embedding_weights.shape))\n",
    "print('{}% not found'.format(round(len(not_found)/vocab_size *100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1340, 1000) (660, 1000)\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.texts_to_sequences(docs_train)\n",
    "X_train = np.array(pad_sequences(X_train, padding='post', maxlen=max_len, truncating='post'))\n",
    "X_test = tokenizer.texts_to_sequences(docs_test)\n",
    "X_test = np.array(pad_sequences(X_test, padding='post', maxlen=max_len, truncating='post'))  \n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 1000, 100)         100100    \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (None, 64)                31680     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 134,149\n",
      "Trainable params: 33,921\n",
      "Non-trainable params: 100,228\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    embedding_layer = Embedding(vocab_size + 1,dim_embedddings, weights=[embedding_weights ],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    model.add(GRU(64, return_sequences=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = get_model()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1340 samples, validate on 660 samples\n",
      "Epoch 1/4\n",
      "1340/1340 [==============================] - 15s 11ms/step - loss: 7.6094 - accuracy: 0.5037 - val_loss: 7.7828 - val_accuracy: 0.4924\n",
      "Epoch 2/4\n",
      "1340/1340 [==============================] - 17s 13ms/step - loss: 7.6094 - accuracy: 0.5037 - val_loss: 7.7828 - val_accuracy: 0.4924\n",
      "Epoch 3/4\n",
      "1340/1340 [==============================] - 18s 13ms/step - loss: 7.6094 - accuracy: 0.5037 - val_loss: 7.7828 - val_accuracy: 0.4924\n",
      "Epoch 4/4\n",
      "1340/1340 [==============================] - 17s 13ms/step - loss: 7.6094 - accuracy: 0.5037 - val_loss: 7.7828 - val_accuracy: 0.4924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f31d27534a8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esm =  EarlyStopping(patience=2, monitor='loss')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, callbacks=[esm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 4s 4ms/step\n",
      "[8.14578327536583, 0.46875]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.6067240834236145, 0.50390625]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.621697887778282, 0.5029296875]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.861279785633087, 0.4873046875]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.561802372336388, 0.5068359375]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.711541011929512, 0.4970703125]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.966096758842468, 0.48046875]\n",
      "1024/1024 [==============================] - 5s 5ms/step\n",
      "[7.711541101336479, 0.4970703125]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a123efa553cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-a123efa553cc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_batches, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##optional method for training the model on a generator\n",
    "def data_gen(docs_train, docs_test,  y_train, y_test, n_batches=1,  batch_size=32):\n",
    "    for i in range(n_batches):\n",
    "        docs_train = np.array(docs_train)\n",
    "        docs_test = np.array(docs_test)\n",
    "        y_train = np.array(y_train)\n",
    "        y_test = np.array(y_test)\n",
    "        \n",
    "        # randomly shuffels training  set\n",
    "        train_index = np.random.choice(np.arange(len(docs_train)), batch_size)\n",
    "        X_train = tokenizer.texts_to_sequences(docs_train[train_index])\n",
    "        X_train = np.array(pad_sequences(X_train, padding='post', maxlen=max_len, truncating='post'))\n",
    "        \n",
    "        #radomly gets the test set\n",
    "        test_index = np.random.choice(np.arange(len(docs_test)), batch_size)\n",
    "        X_test = tokenizer.texts_to_sequences(docs_test[test_index])\n",
    "        X_test = np.array(pad_sequences(X_test, padding='post', maxlen=max_len, truncating='post'))\n",
    "        yield X_train, X_test, y_train[train_index], y_test[test_index]\n",
    "\n",
    "# test data gen\n",
    "X, X_t, y, y_t = next(data_gen(docs_train, docs_test, y_train, y_test, n_batches=2, batch_size=32))\n",
    "print(X.shape, X_t.shape, y.shape, y_t.shape)\n",
    "\n",
    "# training loop\n",
    "def train(n_batches = 20, batch_size = 1024):\n",
    "    g  = data_gen(docs_train, docs_test, y_train, y_test, n_batches=n_batches, batch_size=batch_size)\n",
    "    for i in range(n_batches):\n",
    "        X, X_t, y, y_t = next(g)\n",
    "        model.train_on_batch(X,y)\n",
    "        print(model.evaluate(X_t, y_t))\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
