{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Decision Trees Learn to Make Decisions\n",
    "+ To understand how decision Trees Work, the concept of entropy\n",
    "\n",
    "Information entropy is the average rate at which information is produced by a stochastic source of data.\n",
    "\n",
    "The measure of information entropy associated with each possible data value is the negative logarithm of the probability mass function for the value:\n",
    "+ for a single Class\n",
    "\n",
    "    ent = -p*log(p) \n",
    "    \n",
    "+ sum for all Classes \n",
    "\n",
    " S = − ∑ -p*log(p) \n",
    "\n",
    "##### Function Descriptions\n",
    "+ Entropy Function takes a probabity and return the entropy\n",
    "+ homogeneity function takes and array and calculated the entropy of the array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a entropy function for a single binary class  (0 /1 ) outcome\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def get_entropy(p):\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -p * np.log2(p)\n",
    " \n",
    "\n",
    "def get_homogeneity(x):\n",
    "    n = len(x)\n",
    "    counts = Counter(x).most_common()\n",
    "    p = counts[0][1]/n\n",
    "    return get_entropy(p) + get_entropy(1-p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore How Entropy works on different\n",
    "Entropy is measure between zero and one that measures how mix a variable is.\n",
    "+ contast variables have a entropy of zero\n",
    "+ 50/50 mix has the highest entropy\n",
    "This demonstrates that entropy is highest then variables are perfectly mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy all ones 0.0\n",
      " all zeros 0.0\n",
      "half ones and zeros 1.0\n",
      "Mostly zeros 0.8112781244591328\n",
      "Mostly Ones 0.8112781244591328\n"
     ]
    }
   ],
   "source": [
    "# Contant Distrobutions \n",
    "print('entropy all ones', get_homogeneity(np.ones(100)))\n",
    "# Contant Distrobutions \n",
    "print(' all zeros', get_homogeneity(np.zeros(100)))\n",
    "print('half ones and zeros',get_homogeneity([0,1] * 100))\n",
    "print('Mostly zeros', get_homogeneity([0, 0, 0, 1] * 100 )    )                        \n",
    "print('Mostly Ones', get_homogeneity([1, 1 ,0, 1] * 100 )    )                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information Gain is the Difference in Entropy of a Given Variable before and after a split\n",
    "This creates some data with p(y|x=1) = .25 and p(y|x=2) = .75\n",
    "\n",
    "+ This creates a function to estimate information gain, using difference in homogeneity for each value, for y given x\n",
    "+ This is typically used a split criteria in decision Trees\n",
    "+ The higher the information gain the better the split\n",
    "+ most of the time, frequency weighting is used to gaurd against over fitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.1431558784658321, 2: -0.04556599707503506}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x =  [1,1,1,1,2,2,2,2]\n",
    "y = [0,0,0,1, 0,0,1,1]\n",
    "get_information_gain(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_information_gain(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    n = len(y)\n",
    "    counts = Counter(x).most_common()\n",
    "    p = counts[0][1]/n\n",
    "    entropy_all = get_entropy(p)\n",
    "    if len(counts) == 1:\n",
    "        return entropy_all\n",
    "    else:\n",
    "        for val, c in counts:\n",
    "            y_hat = y[x == val]\n",
    "            \n",
    "            w = len(y_hat)/n # calcuated the wieght\n",
    "            p_hat = c[0][1]/len(y_hat)\n",
    "\n",
    "            info_gain = entropy_all -  w * get_entropy(p_hat)\n",
    "            output.update({c[0]:info_gain})\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.1431558784658321, 2: -0.04556599707503506}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =  [1,1,1,1,2,2,2,2]\n",
    "y = [0,0,0,1, 0,0,1,1]\n",
    "get_information_gain(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity \n",
    "An Alernate way to determin splits is to use Gini Imputiry gain\n",
    "+ Gini Imputurity is as follows\n",
    "\n",
    "+ Gini Impurity is the probability of incorrectly classifying a randomly chosen element in the dataset if it were \n",
    "randomly labeled according to the class distribution in the dataset. G=1∑p(i)∗(1−p(i))\n",
    "+ Gini Gain of .5 suggest a perfect split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impurity(x):\n",
    "    counts = Counter(x).most_common() \n",
    "    n = len(x)\n",
    "    if len(counts) == 1:\n",
    "        p = counts[0][1]/n\n",
    "        return p * (1-p)  +  (1-p) * (1 - (1- p))\n",
    "    else:\n",
    "        output= []\n",
    "        for c in counts:\n",
    "            p = c[1]/n\n",
    "            output.append(p * (1-p) )\n",
    "    return sum(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " all zeros 0.0\n",
      "half ones and zeros 0.5\n",
      "Mostly zeros 0.375\n",
      "Mostly Ones 0.375\n"
     ]
    }
   ],
   "source": [
    "print(' all zeros', get_impurity(np.zeros(100)))\n",
    "print('half ones and zeros',get_impurity([0,1] * 100))\n",
    "print('Mostly zeros', get_impurity([0, 0, 0, 1] * 100 )    )                        \n",
    "print('Mostly Ones', get_impurity([1, 1 ,0, 1] * 100 )    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Gain Function, returns and unweight gini gain of the split, (.5 is perfect and higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gini_gain(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    n = len(y)\n",
    "    counts = Counter(x).most_common()\n",
    "    g = get_impurity(y)\n",
    "    output = {}\n",
    "    for c in counts:\n",
    "        g_after =  get_impurity(y[x == c[0]])\n",
    "        w = c[1]/n # calcuated the wieght\n",
    "        gini_gain = g -   g_after\n",
    "        output.update({c[0]:gini_gain })\n",
    "    return output\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.09375, 2: -0.03125}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x =  [1,1,1,1,2,2,2,2]\n",
    "y = [0,0,0,1, 0,0,1,1]\n",
    "get_gini_gain(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discritizing\n",
    "Most Decision Trees havesome sort of discretizing (binning) method to handel continous varaibles\n",
    "+ Equal Size Binning (same number of elements in each bin)\n",
    "+ Equal Width Binning (each bin is has the same size range between start and end points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges [array([ 91.97987806,  95.94651973,  99.91316141, 103.87980308,\n",
      "       107.84644475])\n",
      " array([ 90.78589642,  94.58096721,  98.37603799, 102.17110877,\n",
      "       105.96617956])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 3.],\n",
       "       [1., 2.],\n",
       "       [2., 2.],\n",
       "       [2., 1.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer as KBinsDiscretizer\n",
    "d = np.random.normal(loc=100, scale = 3,size = (100, 2))\n",
    "est = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='uniform')\n",
    "est.fit(d)\n",
    "print('edges', est.bin_edges_)\n",
    "est.transform(d[[1, 65, 75, 99], :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edges [array([ 97.32589263,  99.32131666, 100.11349301, 100.7940867 ,\n",
      "       102.11009358])\n",
      " array([ 97.6601149 ,  99.32039748, 100.12963615, 100.79573481,\n",
      "       102.26733613])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3., 3.],\n",
       "       [3., 1.],\n",
       "       [1., 3.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = KBinsDiscretizer(n_bins=4, encode='ordinal', strategy='quantile')\n",
    "est.fit(d)\n",
    "print('edges', est.bin_edges_)\n",
    "est.transform(d[[1, 65, 75, 99], :])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
