{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Data Loading and Writing \n",
    "How to use basic spark guide for all the components needed to create Data Science Projects with spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and logging setup\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import  IntegerType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext \n",
    "from pyspark import SparkFiles\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## setups logging\n",
    "import logging\n",
    "\n",
    "try: \n",
    "    logger.debug('logger is up')\n",
    "except:\n",
    "    name = 'pysparkCustomEstimator'\n",
    "    formatter = logging.Formatter(fmt='%(asctime)s -  %(name)s - %(levelname)s  - %(message)s')\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "\n",
    "## create a temp_data directory to work from\n",
    "data_dir = 'temp_data'\n",
    "try:\n",
    "    os.mkdir(data_dir)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting  Spark Context\n",
    "This starts a local spark context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sc = SparkContext(\"local\", \"PysparkHowtoGuide\")\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Data Frome a URL\n",
    "In this case the downloading the age of every congressman from 538\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+---------+----------+---------+------+-------------------+-----+-----+---------+-------------------+----+\n",
      "|congress|chamber|bioguide|firstname|middlename| lastname|suffix|           birthday|state|party|incumbent|          termstart| age|\n",
      "+--------+-------+--------+---------+----------+---------+------+-------------------+-----+-----+---------+-------------------+----+\n",
      "|      80|  house| M000112|   Joseph| Jefferson|Mansfield|  null|1861-02-09 00:00:00|   TX|    D|      Yes|1947-01-03 00:00:00|85.9|\n",
      "|      80|  house| D000448|   Robert|       Lee| Doughton|  null|1863-11-07 00:00:00|   NC|    D|      Yes|1947-01-03 00:00:00|83.2|\n",
      "+--------+-------+--------+---------+----------+---------+------+-------------------+-----+-----+---------+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/fivethirtyeight/data/master/congress-age/congress-terms.csv'\n",
    "sc.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"congress-terms.csv\"), header=True, inferSchema= True)\n",
    "# displays the first two rows\n",
    "df.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(congress,IntegerType,true),StructField(chamber,StringType,true),StructField(bioguide,StringType,true),StructField(firstname,StringType,true),StructField(middlename,StringType,true),StructField(lastname,StringType,true),StructField(suffix,StringType,true),StructField(birthday,TimestampType,true),StructField(state,StringType,true),StructField(party,StringType,true),StructField(incumbent,StringType,true),StructField(termstart,TimestampType,true),StructField(age,DoubleType,true)))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prints out the data schema\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Data to a CSV File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = data_dir + '/congress.csv'\n",
    "df.write.csv(path,  header=True, mode='overwrite') ## write dataframe to csv, using overwrite and including colnams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data From a CSV File\n",
    "Reads in a single csv file with head from the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet vs CSV\n",
    "'Apache Parquet is designed to bring efficient columnar storage of data compared to row-based files like CSV.'\n",
    "+ Pyspark and read and write both csv and parquet, be parquet is much much quicker.\n",
    "+ Parque has rules about columnames and enforces schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Parque Files in Chunks to a Directory\n",
    "here partition is demonstrated where, data is written with paritioning and essentially every partition creates it's own sub directory\n",
    "+ a sub directory of the data_dir is create\n",
    "+ data is written with partitioning \n",
    "\n",
    "Note, with a spark session active, spark essentially maintains a connection csv files that are read into data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._SUCCESS.crc',\n",
       " 'congress=100',\n",
       " 'congress=101',\n",
       " 'congress=102',\n",
       " 'congress=103']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates a new subdirectory inside data dir to store the partition data\n",
    "partitioned_dir = data_dir + '/part'\n",
    "try:\n",
    "    os.mkdir(partitioned_dir )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Choose a column in the data frame to parition by\n",
    "partition_col = 'congress'\n",
    "\n",
    "# writes the data\n",
    "df.write.parquet(partitioned_dir, mode='overwrite', partitionBy=partition_col)\n",
    "\n",
    "logger.info('writing df to {0} using col: {1} for paritioning'.format(partitioned_dir,partition_col  ))\n",
    "# This creates on parquet file for each congress\n",
    "os.listdir(partitioned_dir)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading a Directory of files as one Data Frame\n",
    "Since is is essentually one file for each congress, all the files can be read in once \n",
    "+ enter only the directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format(\"parquet\").option(\"header\", \"true\").load(partitioned_dir)\n",
    "logger.debug('rows form original df {}'.format(df.count()))\n",
    "logger.debug('rows form parque batch files read in df {}'.format(df_read.count()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
