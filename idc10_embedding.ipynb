{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this is test', 'a', 1),\n",
       " ('this is test', 'why', 0),\n",
       " ('this is a', 'test', 1),\n",
       " ('this is a', 'why', 0),\n",
       " ('this is a test', '', 1),\n",
       " ('this is a test', 'a', 0),\n",
       " ('this is a test', 'still', 1),\n",
       " ('this is a test', 'test', 0),\n",
       " ('this is still test', 'a', 1),\n",
       " ('this is still test', 'test', 0),\n",
       " ('this is still a', 'test', 1),\n",
       " ('this is still a', 'still', 0),\n",
       " ('why am still testing', 'i', 1),\n",
       " ('why am still testing', 'a', 0),\n",
       " ('why am i testing', 'still', 1),\n",
       " ('why am i testing', 'still', 0),\n",
       " ('why am i still', 'testing', 1),\n",
       " ('why am i still', 'is', 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "docs = ['this is a test ', 'this is also a test', 'why and I still testing this', 'ok now the test is over']\n",
    "\n",
    "\n",
    "class SkipGram:\n",
    "    def __init__(self, max_len=5, tokenizer=None):\n",
    "        self.max_len = max_len\n",
    "        self.input_len = 2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.words = None\n",
    "        self.probs = None\n",
    "        self.n_words = None\n",
    "        if tokenizer:\n",
    "            self._setup_words()\n",
    "        pass\n",
    "\n",
    "    def _setup_words(self):\n",
    "        self.n_words = sum(self.tokenizer.word_counts.values())\n",
    "        self.words = np.array(list(self.tokenizer.word_counts.keys()))\n",
    "        self.probs = np.array(list(self.tokenizer.word_counts.values()))/self.n_words\n",
    "\n",
    "    def fit(self, docs):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(docs)\n",
    "        self._setup_words()\n",
    "\n",
    "    def _text_to_padded_sequences(self, docs):\n",
    "        seqs = self.tokenizer.texts_to_sequences(docs)\n",
    "        seqs_padded = pad_sequences(seqs, maxlen=self.max_len, padding='post', truncating='post')\n",
    "        return seqs_padded\n",
    "\n",
    "    def _random_choice_int(self, v):\n",
    "        new_word = np.random.choice(self.words, 1, p=self.probs)[0]\n",
    "        output = self.tokenizer.word_index[new_word]\n",
    "        if v == output:\n",
    "            new_word = np.random.choice(self.words, 1, p=self.probs)[0]\n",
    "            output = self.tokenizer.word_index[new_word]\n",
    "        return output\n",
    "\n",
    "    def _missing_word_gen(self, docs):\n",
    "        seqs = self._text_to_padded_sequences(docs)\n",
    "        for seq in seqs:\n",
    "            seq = np.array(seq)\n",
    "            if sum(seq) != 0:\n",
    "                for i in range(self.input_len, self.max_len):\n",
    "                    new_seq = np.delete(seq, i)\n",
    "                    x = np.array([new_seq, new_seq])\n",
    "                    word_index = seq[i]\n",
    "                    new_word_index = self._random_choice_int(word_index)\n",
    "                    context = np.array([[word_index], [new_word_index]])\n",
    "                    label = np.array([[1], [0]])\n",
    "                    yield x, context, label\n",
    "\n",
    "    def array_to_texts(self, x):\n",
    "        return list(s.tokenizer.sequences_to_texts_generator(x))\n",
    "\n",
    "    def transform(self, docs):\n",
    "        array_list = list(self._missing_word_gen(docs))\n",
    "        x = np.vstack([v[0] for v in array_list])\n",
    "        context = np.vstack([v[1] for v in array_list])\n",
    "        labels = np.vstack([v[2] for v in array_list]).flatten()\n",
    "        return x, context, labels\n",
    "\n",
    "docs = ['this is a test', 'this is still a test', 'why am I still testing', '']\n",
    "s = SkipGram()\n",
    "s.fit(docs)\n",
    "x, context, labels = s.transform(docs)\n",
    "list(zip(s.array_to_texts(x), s.array_to_texts(context), labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/kamillamagna/ICD-10-CSV/master/codes.csv', header=None)\n",
    "df.head()\n",
    "docs = df.iloc[:, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00</td>\n",
       "      <td>0</td>\n",
       "      <td>A000</td>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar chol...</td>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar chol...</td>\n",
       "      <td>Cholera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00</td>\n",
       "      <td>1</td>\n",
       "      <td>A001</td>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n",
       "      <td>Cholera due to Vibrio cholerae 01, biovar eltor</td>\n",
       "      <td>Cholera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00</td>\n",
       "      <td>9</td>\n",
       "      <td>A009</td>\n",
       "      <td>Cholera, unspecified</td>\n",
       "      <td>Cholera, unspecified</td>\n",
       "      <td>Cholera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A010</td>\n",
       "      <td>0</td>\n",
       "      <td>A0100</td>\n",
       "      <td>Typhoid fever, unspecified</td>\n",
       "      <td>Typhoid fever, unspecified</td>\n",
       "      <td>Typhoid fever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A010</td>\n",
       "      <td>1</td>\n",
       "      <td>A0101</td>\n",
       "      <td>Typhoid meningitis</td>\n",
       "      <td>Typhoid meningitis</td>\n",
       "      <td>Typhoid fever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1      2                                                  3  \\\n",
       "0   A00  0   A000  Cholera due to Vibrio cholerae 01, biovar chol...   \n",
       "1   A00  1   A001    Cholera due to Vibrio cholerae 01, biovar eltor   \n",
       "2   A00  9   A009                               Cholera, unspecified   \n",
       "3  A010  0  A0100                         Typhoid fever, unspecified   \n",
       "4  A010  1  A0101                                 Typhoid meningitis   \n",
       "\n",
       "                                                   4              5  \n",
       "0  Cholera due to Vibrio cholerae 01, biovar chol...        Cholera  \n",
       "1    Cholera due to Vibrio cholerae 01, biovar eltor        Cholera  \n",
       "2                               Cholera, unspecified        Cholera  \n",
       "3                         Typhoid fever, unspecified  Typhoid fever  \n",
       "4                                 Typhoid meningitis  Typhoid fever  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 6\n",
    "s = SkipGram(max_len=max_len)\n",
    "s.fit(docs)\n",
    "x, context, labels = s.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cholera due vibrio cholerae 01', 'to', 1),\n",
       " ('cholera due vibrio cholerae 01', 'routine', 0),\n",
       " ('cholera due to cholerae 01', 'vibrio', 1),\n",
       " ('cholera due to cholerae 01', 'ulceration', 0),\n",
       " ('cholera due to vibrio 01', 'cholerae', 1),\n",
       " ('cholera due to vibrio 01', 'abuse', 0),\n",
       " ('cholera due to vibrio cholerae', '01', 1),\n",
       " ('cholera due to vibrio cholerae', 'pick', 0),\n",
       " ('cholera due vibrio cholerae 01', 'to', 1),\n",
       " ('cholera due vibrio cholerae 01', 'of', 0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(s.array_to_texts(x[0:10,:]), s.array_to_texts(context[0:10,:]), labels[0:10] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((573632, 5), (573632, 1), (573632,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, context.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7122"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s.tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 5, 10)        71220       input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 1, 10)        71220       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dot_8 (Dot)                     (None, 5, 1)         0           embedding_19[0][0]               \n",
      "                                                                 embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 1, 5)         0           dot_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 5)            0           reshape_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            6           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1)            0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 142,446\n",
      "Trainable params: 142,446\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers.merge import Dot\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "input_len = x.shape[1]\n",
    "vocab_size = len(s.tokenizer.word_index)\n",
    "\n",
    "dim_embedddings =10\n",
    "\n",
    "# inputs\n",
    "w_inputs = Input(shape=(input_len, ), dtype='int32')\n",
    "w = Embedding(vocab_size, dim_embedddings)(w_inputs)\n",
    "\n",
    "# context\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "c  = Embedding(vocab_size, dim_embedddings)(c_inputs)\n",
    "o = Dot(axes=2)([w, c])\n",
    "o = Reshape((1,input_len), input_shape=(input_len, 1))(o)\n",
    "o = Flatten()(o)\n",
    "o = Dense(1)(o)\n",
    "o = Activation('sigmoid')(o)\n",
    "\n",
    "model = Model(inputs=[w_inputs, c_inputs], outputs=o)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "# fit the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "573632/573632 [==============================] - 109s 190us/step - loss: 0.3254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b1dd737a90>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x, context], labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
