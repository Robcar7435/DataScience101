{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Pyspark Estimators and Transformer\n",
    "This tutorial is explains how to setup a custom pyspark estimators and models for use inside pyspark pipelines. In general there are two classes needed, an estimator class and a transformer class. There will be 4 classes use to accomplish this\n",
    "\n",
    "+ HasVocabList, a class for storing a list of strings (persitant in the Transformer)\n",
    "\n",
    "+ InputValidation, test that required columns are present\n",
    "\n",
    "+ Estimator takes a dataframe input and return of Model\n",
    "    * fits the vocabList\n",
    "    * has _fit Method that returns the Transformer\n",
    "+ transformer takes a dataframe input, returns a transformed data frame\n",
    "    * is initilazed by the Estimator.fit() method\n",
    "    * has a persistant vocabList\n",
    "    * has a transform method that returns a new data frame\n",
    "\n",
    "### Test Example:\n",
    "For an example, we're going to create an Estimator and a Model takes a data frame input and creates a column of one hot encoded data (column of 0s and 1s) based whether the data containers a list of strings \n",
    "+ Estimator,\n",
    "    takes data frame as input\n",
    "    inside the inputCol, collects a list of every unique value and stores as vocabList\n",
    "    + param: inputCol string input column name\n",
    "    + param: featureCol string output column name\n",
    "    + returns a Transfomer\n",
    "   \n",
    "inputCol and featuresCol can be imported from pyspark shared parameters, but to store a custom list of strings a new class is needed.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging Explanation\n",
    "here a logger is setup for debugging purposes, use logger.setLevel(logging.DEBUG) to change the level to get more verbose output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and logging setup\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Estimator, Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasFeaturesCol, Param, Params, HasInputCol, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable, MLWritable, MLReadable\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.types import  IntegerType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## setups logging\n",
    "import logging\n",
    "\n",
    "try: \n",
    "    logger.debug('logger is up')\n",
    "except:\n",
    "    name = 'pysparkCustomEstimator'\n",
    "    formatter = logging.Formatter(fmt='%(asctime)s -  %(name)s - %(levelname)s  - %(message)s')\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.WARNING)\n",
    "    logger.addHandler(handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create class to store VocabList\n",
    "+ This create a getVocab method use to store variables \n",
    "+ super class help inhertience in parernt method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HasVocabList(Params):\n",
    "    '''\n",
    "    MixIn for pyspark for pyspark estimators and transforms (holds a list of strings)\n",
    "\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(HasVocabList, self).__init__()\n",
    "        self.vocabList = Param(self, \"vocabList\",  \"list of strings \",\n",
    "                           typeConverter=TypeConverters.toListString)\n",
    "\n",
    "    def getVocabList(self):\n",
    "        return self.getOrDefault(self.vocabList)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an input Validator\n",
    "This is a method that simply checks whether the expected inputCol is in the df being passed in.  It also checks to see whether featureCol already exisits in the df.  Since a featureCol is going to be created in the transform method, if it's already there this throws up an error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputValidation():\n",
    "    def __init__(self):\n",
    "        super(InputValidation, self).__init__()\n",
    "    def validate(self, df):\n",
    "        if self.getInputCol() in df.columns:\n",
    "            logger.debug('inputCol: {} found in df columns'.format(self.getInputCol() ))\n",
    "        else:\n",
    "            raise KeyError('inputCol: {0} not found in df columns {1}'.format( df.columns))\n",
    "        if len(df.columns) == len(list(set(df.columns))):\n",
    "            pass\n",
    "        else:\n",
    "            logger.warning('duplicate columns found in df, may cause unstable behavior')\n",
    "        if self.getFeaturesCol() in df.columns:\n",
    "            raise KeyError('data frame already contains featuresCol {}'.format(self.getFeaturesCol() ))\n",
    "        else:\n",
    "            pass\n",
    "        logger.debug('input Validated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Transform\n",
    "The tranformer takes a df input, uses the vocab list and returns a data frame with a new column of data in it\n",
    "+ When defining the tranfomer class, Inheret the classes of the parameters needed and the Transformer class \n",
    "    + in this case we use our HasVocabList class and HasInputCol, HasFeaturesCol from pyspark\n",
    "+ keyword_only decorartor saves actual input keyword arguments in `_input_kwargs`\n",
    "+ intit and set params, essentailly save all the keyword args to self.\n",
    "+ _tranform takes a data frame and returns a data from\n",
    "    + Uses a udf (user defined function) to apply the _look_up helper function across the rows of the spark data frame\n",
    "+ _look_up is a helper function that takes a string and return 0 or 1 depending on whether that string is in the list\n",
    "\n",
    "+ _input_validation method is use to ensure the need columns are present, and no duplicate columns are use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabListTransformer(Transformer, HasInputCol, HasFeaturesCol, HasVocabList, InputValidation,  \n",
    "                           DefaultParamsReadable, DefaultParamsWritable, MLReadable, MLWritable):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self,  inputCol=None, featuresCol=None, vocabList=None):\n",
    "        super(VocabListTransformer, self).__init__()\n",
    "        logger.debug('VocabTranformerClass Initialized')\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, featuresCol=None, vocabList=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _look_up(self, x):\n",
    "        if x in self.getVocabList():\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _transform(self, df):\n",
    "        logger.debug('VocabTranformerClass transform method called')\n",
    "        self.validate(df)\n",
    "        # creates an on the fly udf  that applies a the look up help function to every row of the data frame\n",
    "        tranform_udf = udf(lambda x: self._look_up(x), IntegerType())\n",
    "        # creates a new data frame with a new column with the results on the look_up on each row\n",
    "        df = df.withColumn(self.getFeaturesCol(), tranform_udf(self.getInputCol()))\n",
    "        return df \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Estimator\n",
    "The estimator uses inhertiance to store variables, and use a standard interface to extract them back.\n",
    "\n",
    "+ When defining the class, Inheret the classes of the parameters needed\n",
    "    + in this case we use our HasVocabList class and HasInputCol, HasFeaturesCol from pyspark\n",
    "+ keyword_only decorartor saves actual input keyword arguments in `_input_kwargs`\n",
    "+ input_validation method is use to ensure the need columns are present, and no duplicate columns are use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabListEstimator(Estimator, HasInputCol, HasFeaturesCol, HasVocabList,  InputValidation):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, featuresCol=None):\n",
    "        super(VocabListEstimator, self).__init__()\n",
    "        logger.debug('VocabEstimatorClass Initialized')\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)  \n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, featuresCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _fit(self, df):\n",
    "        logger.debug('VocabTranformerClass fit method called ')\n",
    "        self.validate(df)\n",
    "        self._get_vocab(df)\n",
    "        ## returns the VocabListTransformer, with all the params from \n",
    "        return VocabListTransformer(inputCol=self.getInputCol(),\n",
    "                                    featuresCol=self.getFeaturesCol(),\n",
    "                                    vocabList=self.getVocabList())\n",
    "    def _get_vocab(self, df):\n",
    "        vocabList =  np.ravel(df.select(self.getInputCol()).distinct().collect())\n",
    "        vocabList = [str(v) for v in vocabList]\n",
    "        self._set(vocabList=vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using and Testing the Methods\n",
    "Fire up a local spark context!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Some Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| pet|\n",
      "+----+\n",
      "| cat|\n",
      "| dog|\n",
      "|fish|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe for fitting\n",
    "df = spark.createDataFrame(pd.DataFrame({'pet':['cat', 'dog', 'fish']}))\n",
    "# create a dataframe for testing, with a new unseen category.\n",
    "df_new = spark.createDataFrame(pd.DataFrame({'pet':['cat', 'dog', 'fish', 'frog']}))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator / Transformer Usage\n",
    "+ Estimate is intially fit, and returns a transformer object (including the fitted vocab list)\n",
    "+ This is then used to transform unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| pet|SeenBefore|\n",
      "+----+----------+\n",
      "| cat|         1|\n",
      "| dog|         1|\n",
      "|fish|         1|\n",
      "|frog|         0|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Intiailizes the Estimator\n",
    "trans = VocabListEstimator(inputCol='pet', featuresCol='SeenBefore')\n",
    "# Fits the Estimator and returns the model\n",
    "trans = trans.fit(df)\n",
    "# transforms new and unseen data \n",
    "df_transformed = trans.transform(df_new)\n",
    "df_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persistance and Integration to a Pyspark Pipeline \n",
    "To use this method inside a pyspark pipeline, just intialized a PipelineModel with a list of stages including the transformer ovject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pet: string, SeenBefore: int]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages = [trans]\n",
    "pipe = PipelineModel(stages=stages)\n",
    "pipe.transform(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and Save of the Transformer\n",
    "Since  DefaultParamsReadable, DefaultParamsWritable, MLReadable, MLWritable were inherited on the definition of the\n",
    "transformer class, then the transform should be persistant and savable and loadable inside a pyspark pipeline.\n",
    "+ the transformer needs at attr on main, to be loadable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| pet|SeenBefore|\n",
      "+----+----------+\n",
      "| cat|         1|\n",
      "| dog|         1|\n",
      "|fish|         1|\n",
      "|frog|         0|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sets the class on main, so that it can be loaded \n",
    "m = __import__(\"__main__\")\n",
    "setattr(m, 'VocabListTransformer',VocabListTransformer)\n",
    "        \n",
    "path = '_testPipeline.pipe'\n",
    "# save the tranformer\n",
    "pipe.write().overwrite().save(path)\n",
    "# load the transformer\n",
    "pipe_loaded = PipelineModel.load(path)\n",
    "# test the loaded transformer\n",
    "pipe_loaded.transform(df_new).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
